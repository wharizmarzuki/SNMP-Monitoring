Based on the analysis of `backend/app/api/v1/endpoints/query.py` and `backend/app/core/models.py`, here is the comprehensive fix plan.

### **Executive Summary**

The issue stems from a "dirty read" of negative values caused by 64-bit counters wrapping, combined with a calculation order bug in Python.

  * **Severity:** High (Data Accuracy)
  * **Root Cause:** 32-bit math applied to 64-bit counters + calculating utilization before sanitizing negative values.
  * **Solution:** Patch the SQL query to discard invalid wraps and reorder the Python logic to sanitize inputs before calculation.

-----

### **Phase 1: Immediate Hotfix (Apply to `query.py`)**

This phase fixes the negative numbers immediately without requiring a database migration.

#### **Step 1: Fix the Python Calculation Order**

You are currently cleaning the BPS value for *display* but using the *raw negative value* for the percentage calculation.

**File:** `backend/app/api/v1/endpoints/query.py`  
**Location:** Inside `get_device_utilization` function, around line 355.

**Change this:**

```python
    for row in results_list:
        # Calculate utilization percentages
        utilization_in_pct = None
        # ... (rest of logic using dirty row.inbound_bps)
```

**To this:**

```python
    for row in results_list:
        # 1. SANITIZE FIRST: Clamp negative values to 0
        safe_inbound_bps = row.inbound_bps if row.inbound_bps and row.inbound_bps > 0 else 0.0
        safe_outbound_bps = row.outbound_bps if row.outbound_bps and row.outbound_bps > 0 else 0.0

        # 2. Calculate utilization percentages using SAFE values
        utilization_in_pct = 0.0
        utilization_out_pct = 0.0
        max_utilization_pct = 0.0

        if row.total_capacity_bps and row.total_capacity_bps > 0:
            utilization_in_pct = (safe_inbound_bps / row.total_capacity_bps) * 100
            utilization_out_pct = (safe_outbound_bps / row.total_capacity_bps) * 100
            max_utilization_pct = max(utilization_in_pct, utilization_out_pct)

        output.append(
            schemas.DeviceUtilizationDatapoint(
                device_id=row.device_id,
                hostname=row.hostname,
                ip_address=row.ip_address,
                timestamp=to_utc_iso(row.timestamp),
                inbound_bps=safe_inbound_bps,     # Use sanitized variable
                outbound_bps=safe_outbound_bps,   # Use sanitized variable
                total_capacity_bps=row.total_capacity_bps,
                utilization_in_pct=utilization_in_pct,
                utilization_out_pct=utilization_out_pct,
                max_utilization_pct=max_utilization_pct
            )
        )
```

#### **Step 2: Patch SQL Wrap Logic**

The current SQL logic assumes all counters are 32-bit. When a 64-bit counter resets, the 32-bit math generates a massive negative number. We must tell SQL to return `0` if the wrap calculation fails (results in a negative).

**File:** `backend/app/api/v1/endpoints/query.py`  
**Location:** `get_device_utilization` (lines 310-318) AND `get_network_throughput` (lines 200-208).

**Change this logic:**

```python
    # Existing 32-bit assumption
    delta_in_expr = case(
        (lag_subquery.c.octets_in >= lag_subquery.c.prev_octets_in,
         lag_subquery.c.octets_in - lag_subquery.c.prev_octets_in),
        else_=(COUNTER32_MAX - lag_subquery.c.prev_octets_in + lag_subquery.c.octets_in)
    ).label("delta_in")
```

**To this (Safer Logic):**

```python
    # Logic: If current < prev, check if the 32-bit wrap fix works. 
    # If the result is still negative, it means it was likely a 64-bit wrap or reset -> return 0.
    
    wrap_fix_in = COUNTER32_MAX - lag_subquery.c.prev_octets_in + lag_subquery.c.octets_in
    wrap_fix_out = COUNTER32_MAX - lag_subquery.c.prev_octets_out + lag_subquery.c.octets_out

    delta_in_expr = case(
        (lag_subquery.c.octets_in >= lag_subquery.c.prev_octets_in,
         lag_subquery.c.octets_in - lag_subquery.c.prev_octets_in),
        (wrap_fix_in < 0, 0),  # <--- DISCARD INVALID WRAPS
        else_=wrap_fix_in
    ).label("delta_in")

    delta_out_expr = case(
        (lag_subquery.c.octets_out >= lag_subquery.c.prev_octets_out,
         lag_subquery.c.octets_out - lag_subquery.c.prev_octets_out),
        (wrap_fix_out < 0, 0), # <--- DISCARD INVALID WRAPS
        else_=wrap_fix_out
    ).label("delta_out")
```

-----

### **Phase 2: Data Integrity (Recommended Long-term)**

Using `Float` for byte counters causes precision loss on high-speed links (e.g., 40Gbps+). You should switch to `BigInteger`.

#### **Step 3: Update Data Models**

**File:** `backend/app/core/models.py`  
**Location:** `InterfaceMetric` class (lines 111-116).

**Change:**

```python
    # Counters - Change from Float to BigInteger
    octets_in: Mapped[int | None] = mapped_column(BigInteger)
    octets_out: Mapped[int | None] = mapped_column(BigInteger)
    errors_in: Mapped[int | None] = mapped_column(BigInteger)
    errors_out: Mapped[int | None] = mapped_column(BigInteger)
    discards_in: Mapped[int | None] = mapped_column(BigInteger)
    discards_out: Mapped[int | None] = mapped_column(BigInteger)
```

#### **Step 4: Database Migration**

Since you changed the model, you must run a migration (using Alembic or raw SQL) to convert the columns in your database:

```sql
ALTER TABLE interface_metrics 
ALTER COLUMN octets_in TYPE BIGINT,
ALTER COLUMN octets_out TYPE BIGINT;
```

-----

### **Phase 3: Visualization Clean-up (Optional)**

To prevent "flat lines" at 0% for unused interfaces, filter them out.

**File:** `backend/app/api/v1/endpoints/query.py`  
**Location:** `get_device_utilization` inside `lag_subquery` filter (line 297).

**Add this filter:**

```python
    ).filter(
        models.Interface.speed_bps != None,
        # Only show interfaces that are administratively UP (1)
        models.InterfaceMetric.admin_status == 1, 
        models.InterfaceMetric.timestamp >= start_time,
        models.InterfaceMetric.timestamp <= end_time
    ).subquery()
```